{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BlueTrails AI - Water Quality Prediction Inference\n",
        "\n",
        "**Purpose**: Generate future water quality predictions and store them in the database\n",
        "\n",
        "**Process**:\n",
        "1. Load best models for each parameter (hybrid V2/V4 approach)\n",
        "2. Generate future dates for prediction\n",
        "3. Prepare features for all sites\n",
        "4. Run predictions using best models\n",
        "5. Bulk insert predictions to database\n",
        "\n",
        "**Best Models** (based on V4 weighted scoring analysis):\n",
        "- **CHL_A**: V2 MLP (R²=0.21, MAE=3.04)\n",
        "- **Turbidity**: V4 Random Forest (R²=0.44, MAE=2.62)\n",
        "- **DO_mg_l**: V4 Ridge Regression (R²=0.38, MAE=1.32)\n",
        "- **N_TOTAL**: V2 MLP (R²=0.30, MAE=194.22)\n",
        "- **Temperature**: V4 Random Forest (R²=0.90, MAE=0.97)\n",
        "\n",
        "**Date Range**: Configurable start_date and end_date parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries loaded successfully!\n",
            "PyTorch version: 2.8.0+cpu\n",
            "Device: CPU\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from datetime import datetime, timedelta\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import uuid\n",
        "\n",
        "# PyTorch for V2 models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(34328637)\n",
        "torch.manual_seed(34328637)\n",
        "\n",
        "print(f\"✓ Libraries loaded successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "INFERENCE CONFIGURATION\n",
            "======================================================================\n",
            "Date Range: 2025-10-03 to 2025-10-31\n",
            "Frequency: Every 1 days\n",
            "Model Version: hybrid_v2_v4_2025\n",
            "\n",
            "Best Models:\n",
            "  chl_a           - V2_MLP               (R²=0.2103)\n",
            "  turbidity       - V4_RandomForest      (R²=0.4395)\n",
            "  do_mg_l         - V4_Ridge             (R²=0.3838)\n",
            "  n_total         - V2_MLP               (R²=0.2992)\n",
            "  temperature     - V4_RandomForest      (R²=0.8982)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Modify these parameters as needed\n",
        "# ============================================================================\n",
        "\n",
        "# Prediction date range\n",
        "START_DATE = '2025-10-03'  # Start date for predictions\n",
        "END_DATE = '2025-10-31'    # End date for predictions\n",
        "\n",
        "# Prediction frequency (days between predictions)\n",
        "FREQUENCY_DAYS = 1\n",
        "\n",
        "# Model version identifier\n",
        "MODEL_VERSION = 'hybrid_v2_v4_2025'\n",
        "\n",
        "# Best models configuration (based on weighted scoring analysis)\n",
        "BEST_MODELS = {\n",
        "    'chl_a': {\n",
        "        'version': 'v2',\n",
        "        'model_file': '../model/internal/v2/chl_a_model.pth',\n",
        "        'model_name': 'V2_MLP',\n",
        "        'param_db_name': 'CHL_A',\n",
        "        'r2': 0.2103,\n",
        "    },\n",
        "    'turbidity': {\n",
        "        'version': 'v4',\n",
        "        'model_file': '../model/internal/v4/turbidity_classical_ml.pkl',\n",
        "        'model_name': 'V4_RandomForest',\n",
        "        'param_db_name': 'Turb',\n",
        "        'r2': 0.4395,\n",
        "    },\n",
        "    'do_mg_l': {\n",
        "        'version': 'v4',\n",
        "        'model_file': '../model/internal/v4/do_mg_l_classical_ml.pkl',\n",
        "        'model_name': 'V4_Ridge',\n",
        "        'param_db_name': 'DO_mg',\n",
        "        'r2': 0.3838,\n",
        "    },\n",
        "    'n_total': {\n",
        "        'version': 'v2',\n",
        "        'model_file': '../model/internal/v2/n_total_model.pth',\n",
        "        'model_name': 'V2_MLP',\n",
        "        'param_db_name': 'N_TOTAL',\n",
        "        'r2': 0.2992,\n",
        "    },\n",
        "    'temperature': {\n",
        "        'version': 'v4',\n",
        "        'model_file': '../model/internal/v4/temperature_classical_ml.pkl',\n",
        "        'model_name': 'V4_RandomForest',\n",
        "        'param_db_name': 'Temperature',\n",
        "        'r2': 0.8982,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INFERENCE CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
        "print(f\"Frequency: Every {FREQUENCY_DAYS} days\")\n",
        "print(f\"Model Version: {MODEL_VERSION}\")\n",
        "print(f\"\\nBest Models:\")\n",
        "for param, config in BEST_MODELS.items():\n",
        "    print(f\"  {param:15} - {config['model_name']:20} (R²={config['r2']:.4f})\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Connect to Supabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Connected to Supabase\n"
          ]
        }
      ],
      "source": [
        "load_dotenv('../.env')\n",
        "\n",
        "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
        "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
        "\n",
        "if not SUPABASE_URL or not SUPABASE_KEY:\n",
        "    raise ValueError(\"Please set SUPABASE_URL and SUPABASE_KEY in .env file\")\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "print(\"✓ Connected to Supabase\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Historical Data for Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 5853 historical measurements\n",
            "✓ Found 21 monitoring sites\n",
            "✓ Parameter ID mapping: {'CHL_A': 1, 'Secchi_depth_m': 2, 'DO_mg': 3, 'FL': 4, 'PAR': 5, 'Sal': 6, 'Temp': 7, 'Turb': 8, 'DO_sat': 9, 'Temperature': 10, 'pH': 11, 'TSS': 12, 'N_NH3': 13, 'N_NO2': 14, 'N_NO3': 15, 'N_NOX': 16, 'N_TOTAL': 17, 'P_PO4': 18, 'P_TOTAL': 19, 'SI': 20}\n",
            "✓ Surface type ID: 1\n"
          ]
        }
      ],
      "source": [
        "# Fetch historical data\n",
        "response = supabase.table('v_epa_measurements_wide').select('*').execute()\n",
        "df_historical = pd.DataFrame(response.data)\n",
        "\n",
        "# Fetch sites\n",
        "response_sites = supabase.table('v_epa_sites').select('*').execute()\n",
        "df_sites = pd.DataFrame(response_sites.data)\n",
        "\n",
        "# Fetch param IDs for database insertion\n",
        "response_params = supabase.table('epa_param').select('id, name').execute()\n",
        "df_params = pd.DataFrame(response_params.data)\n",
        "param_id_map = dict(zip(df_params['name'], df_params['id']))\n",
        "\n",
        "# Fetch type ID for 'surface'\n",
        "response_types = supabase.table('epa_type').select('id, name').execute()\n",
        "df_types = pd.DataFrame(response_types.data)\n",
        "surface_type_id = df_types[df_types['name'] == 'surface']['id'].values[0]\n",
        "\n",
        "print(f\"✓ Loaded {len(df_historical)} historical measurements\")\n",
        "print(f\"✓ Found {len(df_sites)} monitoring sites\")\n",
        "print(f\"✓ Parameter ID mapping: {param_id_map}\")\n",
        "print(f\"✓ Surface type ID: {surface_type_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Historical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Prepared 4492 surface measurements\n",
            "✓ Date range: 1984-07-19 00:00:00 to 2025-06-20 00:00:00\n"
          ]
        }
      ],
      "source": [
        "# Filter to surface measurements\n",
        "df = df_historical[df_historical['measurement_type'] == 'surface'].copy()\n",
        "\n",
        "# Merge with site metadata\n",
        "df = df.merge(df_sites[['site_id', 'latitude', 'longitude', 'water_body_name']], \n",
        "              on='site_id', how='left', suffixes=('', '_site'))\n",
        "\n",
        "df['latitude'] = df['latitude_site'].fillna(df['latitude'])\n",
        "df['longitude'] = df['longitude_site'].fillna(df['longitude'])\n",
        "df = df.drop(['latitude_site', 'longitude_site'], axis=1, errors='ignore')\n",
        "\n",
        "# Convert date\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Define target parameters (same as training)\n",
        "df['temperature'] = df['temp_sensor'].fillna(df['temp_lab'])\n",
        "TARGET_PARAMS = ['chl_a', 'turbidity', 'do_mg_l', 'n_total', 'temperature']\n",
        "\n",
        "print(f\"✓ Prepared {len(df)} surface measurements\")\n",
        "print(f\"✓ Date range: {df['date'].min()} to {df['date'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Engineering (Same as Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Feature engineering complete\n",
            "✓ Encoded 21 sites and 3 water bodies\n"
          ]
        }
      ],
      "source": [
        "# Encode categorical features (fit on historical data)\n",
        "site_encoder = LabelEncoder()\n",
        "df['site_id_encoded'] = site_encoder.fit_transform(df['site_id'])\n",
        "\n",
        "water_body_encoder = LabelEncoder()\n",
        "df['water_body_encoded'] = water_body_encoder.fit_transform(df['water_body_name'].fillna('Unknown'))\n",
        "\n",
        "# Sort by site and date\n",
        "df = df.sort_values(['site_id', 'date']).reset_index(drop=True)\n",
        "\n",
        "# Calculate site statistics (needed for future predictions)\n",
        "site_stats = df.groupby('site_id')[TARGET_PARAMS].agg(['mean', 'std']).reset_index()\n",
        "site_stats.columns = ['site_id'] + [f'{param}_{stat}' for param in TARGET_PARAMS for stat in ['mean', 'std']]\n",
        "\n",
        "# Get last known values per site (for lag features)\n",
        "last_values = df.sort_values('date').groupby('site_id')[TARGET_PARAMS].last().reset_index()\n",
        "last_values.columns = ['site_id'] + [f'{param}_last' for param in TARGET_PARAMS]\n",
        "\n",
        "print(\"✓ Feature engineering complete\")\n",
        "print(f\"✓ Encoded {df['site_id'].nunique()} sites and {df['water_body_name'].nunique()} water bodies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate Future Dates for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "PREDICTION DATES\n",
            "======================================================================\n",
            "Total dates: 29\n",
            "Dates: ['2025-10-03', '2025-10-04', '2025-10-05', '2025-10-06', '2025-10-07', '2025-10-08', '2025-10-09', '2025-10-10', '2025-10-11', '2025-10-12', '2025-10-13', '2025-10-14', '2025-10-15', '2025-10-16', '2025-10-17', '2025-10-18', '2025-10-19', '2025-10-20', '2025-10-21', '2025-10-22', '2025-10-23', '2025-10-24', '2025-10-25', '2025-10-26', '2025-10-27', '2025-10-28', '2025-10-29', '2025-10-30', '2025-10-31']\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate prediction dates\n",
        "start_date = pd.to_datetime(START_DATE)\n",
        "end_date = pd.to_datetime(END_DATE)\n",
        "\n",
        "prediction_dates = pd.date_range(start=start_date, end=end_date, freq=f'{FREQUENCY_DAYS}D')\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"PREDICTION DATES\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total dates: {len(prediction_dates)}\")\n",
        "print(f\"Dates: {list(prediction_dates.strftime('%Y-%m-%d'))}\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create Prediction Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Created prediction dataset with 609 rows\n",
            "  Sites: 21\n",
            "  Dates: 29\n",
            "  Total predictions: 3045 (across all parameters)\n"
          ]
        }
      ],
      "source": [
        "# Create cartesian product of sites × dates\n",
        "sites_list = df_sites['site_id'].unique()\n",
        "prediction_data = []\n",
        "\n",
        "for site_id in sites_list:\n",
        "    for pred_date in prediction_dates:\n",
        "        prediction_data.append({\n",
        "            'site_id': site_id,\n",
        "            'date': pred_date,\n",
        "        })\n",
        "\n",
        "df_predict = pd.DataFrame(prediction_data)\n",
        "\n",
        "# Merge with site metadata\n",
        "df_predict = df_predict.merge(df_sites[['site_id', 'latitude', 'longitude', 'water_body_name']], \n",
        "                               on='site_id', how='left')\n",
        "\n",
        "# Merge with site statistics\n",
        "df_predict = df_predict.merge(site_stats, on='site_id', how='left')\n",
        "\n",
        "# Merge with last known values (for lag features)\n",
        "df_predict = df_predict.merge(last_values, on='site_id', how='left')\n",
        "\n",
        "# Create temporal features\n",
        "df_predict['year'] = df_predict['date'].dt.year\n",
        "df_predict['month'] = df_predict['date'].dt.month\n",
        "df_predict['day_of_year'] = df_predict['date'].dt.dayofyear\n",
        "df_predict['season'] = df_predict['month'].apply(lambda m: \n",
        "    1 if m in [12, 1, 2] else 2 if m in [3, 4, 5] else 3 if m in [6, 7, 8] else 4\n",
        ")\n",
        "\n",
        "# Cyclic encoding\n",
        "df_predict['month_sin'] = np.sin(2 * np.pi * df_predict['month'] / 12)\n",
        "df_predict['month_cos'] = np.cos(2 * np.pi * df_predict['month'] / 12)\n",
        "df_predict['day_sin'] = np.sin(2 * np.pi * df_predict['day_of_year'] / 365)\n",
        "df_predict['day_cos'] = np.cos(2 * np.pi * df_predict['day_of_year'] / 365)\n",
        "\n",
        "# Encode categorical features\n",
        "df_predict['site_id_encoded'] = site_encoder.transform(df_predict['site_id'])\n",
        "df_predict['water_body_encoded'] = water_body_encoder.transform(df_predict['water_body_name'].fillna('Unknown'))\n",
        "\n",
        "# Create lag features from last known values\n",
        "for param in TARGET_PARAMS:\n",
        "    df_predict[f'{param}_lag1'] = df_predict[f'{param}_last']\n",
        "\n",
        "print(f\"\\n✓ Created prediction dataset with {len(df_predict)} rows\")\n",
        "print(f\"  Sites: {len(sites_list)}\")\n",
        "print(f\"  Dates: {len(prediction_dates)}\")\n",
        "print(f\"  Total predictions: {len(sites_list) * len(prediction_dates) * len(TARGET_PARAMS)} (across all parameters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Define Feature List (Same as Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total features: 26\n",
            "  Core: 11\n",
            "  Site Stats: 10\n",
            "  Lag: 5\n",
            "\n",
            "✓ All features imputed\n"
          ]
        }
      ],
      "source": [
        "# Core features\n",
        "CORE_FEATURES = [\n",
        "    'site_id_encoded', 'water_body_encoded', 'latitude', 'longitude',\n",
        "    'year', 'month', 'season',\n",
        "    'month_sin', 'month_cos', 'day_sin', 'day_cos',\n",
        "]\n",
        "\n",
        "# Site statistics\n",
        "STAT_FEATURES = [col for col in df_predict.columns if any(stat in col for stat in ['_mean', '_std'])]\n",
        "\n",
        "# Lag features\n",
        "LAG_FEATURES = [f'{param}_lag1' for param in TARGET_PARAMS]\n",
        "\n",
        "# All features\n",
        "ALL_FEATURES = CORE_FEATURES + STAT_FEATURES + LAG_FEATURES\n",
        "\n",
        "print(f\"Total features: {len(ALL_FEATURES)}\")\n",
        "print(f\"  Core: {len(CORE_FEATURES)}\")\n",
        "print(f\"  Site Stats: {len(STAT_FEATURES)}\")\n",
        "print(f\"  Lag: {len(LAG_FEATURES)}\")\n",
        "\n",
        "# Impute missing values (same as training)\n",
        "core_imputer = SimpleImputer(strategy='median')\n",
        "df_predict[CORE_FEATURES] = core_imputer.fit_transform(df_predict[CORE_FEATURES])\n",
        "\n",
        "stat_imputer = SimpleImputer(strategy='median')\n",
        "df_predict[STAT_FEATURES] = stat_imputer.fit_transform(df_predict[STAT_FEATURES])\n",
        "\n",
        "lag_imputer = SimpleImputer(strategy='median')\n",
        "df_predict[LAG_FEATURES] = lag_imputer.fit_transform(df_predict[LAG_FEATURES])\n",
        "\n",
        "print(\"\\n✓ All features imputed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Define V2 MLP Architecture (for loading)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ V2 MLP architecture defined\n"
          ]
        }
      ],
      "source": [
        "class MLPRegressor(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron for regression (V2 architecture).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, dropout=0.3):\n",
        "        super(MLPRegressor, self).__init__()\n",
        "        \n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim1),\n",
        "            nn.BatchNorm1d(hidden_dim1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.BatchNorm1d(hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Linear(hidden_dim2, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x).squeeze()\n",
        "\n",
        "print(\"✓ V2 MLP architecture defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Load Models and Run Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING MODELS AND RUNNING PREDICTIONS\n",
            "======================================================================\n",
            "\n",
            "CHL_A:\n",
            "  Model: V2_MLP\n",
            "  Version: v2\n",
            "  File: ../model/internal/v2/chl_a_model.pth\n",
            "  ✓ Generated 609 predictions\n",
            "    Min: 0.2974\n",
            "    Max: 21.8591\n",
            "    Mean: 3.3688\n",
            "\n",
            "TURBIDITY:\n",
            "  Model: V4_RandomForest\n",
            "  Version: v4\n",
            "  File: ../model/internal/v4/turbidity_classical_ml.pkl\n",
            "  ✓ Generated 609 predictions\n",
            "    Min: 0.4551\n",
            "    Max: 26.9733\n",
            "    Mean: 4.0541\n",
            "\n",
            "DO_MG_L:\n",
            "  Model: V4_Ridge\n",
            "  Version: v4\n",
            "  File: ../model/internal/v4/do_mg_l_classical_ml.pkl\n",
            "  ✓ Generated 609 predictions\n",
            "    Min: 7.4266\n",
            "    Max: 50.0391\n",
            "    Mean: 10.4020\n",
            "\n",
            "N_TOTAL:\n",
            "  Model: V2_MLP\n",
            "  Version: v2\n",
            "  File: ../model/internal/v2/n_total_model.pth\n",
            "  ✓ Generated 609 predictions\n",
            "    Min: 101.1961\n",
            "    Max: 832.9276\n",
            "    Mean: 305.9594\n",
            "\n",
            "TEMPERATURE:\n",
            "  Model: V4_RandomForest\n",
            "  Version: v4\n",
            "  File: ../model/internal/v4/temperature_classical_ml.pkl\n",
            "  ✓ Generated 609 predictions\n",
            "    Min: 14.2728\n",
            "    Max: 17.5036\n",
            "    Mean: 16.1415\n",
            "\n",
            "======================================================================\n",
            "✓ ALL PREDICTIONS GENERATED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Storage for predictions\n",
        "predictions = {}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"LOADING MODELS AND RUNNING PREDICTIONS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for param, config in BEST_MODELS.items():\n",
        "    print(f\"\\n{param.upper()}:\")\n",
        "    print(f\"  Model: {config['model_name']}\")\n",
        "    print(f\"  Version: {config['version']}\")\n",
        "    print(f\"  File: {config['model_file']}\")\n",
        "    \n",
        "    if config['version'] == 'v2':\n",
        "        # Load V2 PyTorch model\n",
        "        model_artifact = torch.load(config['model_file'], map_location=device, weights_only=False)\n",
        "        \n",
        "        # Extract components\n",
        "        scaler_X = model_artifact['scaler_X']\n",
        "        scaler_y = model_artifact['scaler_y']\n",
        "        \n",
        "        # Initialize model\n",
        "        input_dim = len(ALL_FEATURES)\n",
        "        model = MLPRegressor(input_dim, hidden_dim1=128, hidden_dim2=64, dropout=0.3).to(device)\n",
        "        model.load_state_dict(model_artifact['model_state_dict'])\n",
        "        model.eval()\n",
        "        \n",
        "        # Prepare data\n",
        "        X = df_predict[ALL_FEATURES].values\n",
        "        X_scaled = scaler_X.transform(X)\n",
        "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
        "        \n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            y_pred_scaled = model(X_tensor).cpu().numpy()\n",
        "        \n",
        "        # Inverse transform\n",
        "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "        \n",
        "    elif config['version'] == 'v4':\n",
        "        # Load V4 classical ML model\n",
        "        with open(config['model_file'], 'rb') as f:\n",
        "            model_artifact = pickle.load(f)\n",
        "        \n",
        "        model = model_artifact['model']\n",
        "        scaler_X = model_artifact['scaler_X']\n",
        "        poly = model_artifact.get('poly', None)\n",
        "        \n",
        "        # Prepare data\n",
        "        X = df_predict[ALL_FEATURES].values\n",
        "        X_scaled = scaler_X.transform(X)\n",
        "        \n",
        "        # Apply polynomial features if needed\n",
        "        if poly is not None:\n",
        "            X_scaled = poly.transform(X_scaled)\n",
        "        \n",
        "        # Predict\n",
        "        y_pred = model.predict(X_scaled)\n",
        "    \n",
        "    # Store predictions\n",
        "    predictions[param] = y_pred\n",
        "    \n",
        "    print(f\"  ✓ Generated {len(y_pred)} predictions\")\n",
        "    print(f\"    Min: {y_pred.min():.4f}\")\n",
        "    print(f\"    Max: {y_pred.max():.4f}\")\n",
        "    print(f\"    Mean: {y_pred.mean():.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✓ ALL PREDICTIONS GENERATED SUCCESSFULLY!\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Prepare Data for Database Insertion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction Run ID: 619d8e03-212c-4145-a6bd-71439df9f28f\n",
            "Total predictions: 3045\n"
          ]
        }
      ],
      "source": [
        "# Generate prediction run ID\n",
        "prediction_run_id = str(uuid.uuid4())\n",
        "\n",
        "# Create prediction metadata\n",
        "prediction_metadata = {\n",
        "    'prediction_run_id': prediction_run_id,\n",
        "    'model_version': MODEL_VERSION,\n",
        "    'start_date': START_DATE,\n",
        "    'end_date': END_DATE,\n",
        "    'site_count': len(sites_list),\n",
        "    'prediction_count': len(df_predict) * len(TARGET_PARAMS),\n",
        "    'model_info': {\n",
        "        'best_models': {k: {'model_name': v['model_name'], 'r2': v['r2']} for k, v in BEST_MODELS.items()},\n",
        "        'frequency_days': FREQUENCY_DAYS,\n",
        "        'features': ALL_FEATURES,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"Prediction Run ID: {prediction_run_id}\")\n",
        "print(f\"Total predictions: {len(df_predict) * len(TARGET_PARAMS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Prepared 3045 prediction records for insertion\n",
            "\n",
            "Sample records:\n",
            "  2316 | 2025-10-03 | Param ID 1 | Value: 4.2296 | Confidence: 0.2103 | Model: V2_MLP\n",
            "  2316 | 2025-10-03 | Param ID 8 | Value: 2.5955 | Confidence: 0.4395 | Model: V4_RandomForest\n",
            "  2316 | 2025-10-03 | Param ID 3 | Value: 9.8049 | Confidence: 0.3838 | Model: V4_Ridge\n",
            "  2316 | 2025-10-03 | Param ID 17 | Value: 370.6689 | Confidence: 0.2992 | Model: V2_MLP\n",
            "  2316 | 2025-10-03 | Param ID 10 | Value: 16.1066 | Confidence: 0.8982 | Model: V4_RandomForest\n"
          ]
        }
      ],
      "source": [
        "# Prepare prediction records for bulk insert\n",
        "prediction_records = []\n",
        "\n",
        "for idx, row in df_predict.iterrows():\n",
        "    site_id = row['site_id']\n",
        "    date = row['date'].strftime('%Y-%m-%d')\n",
        "    \n",
        "    for param, y_pred_value in predictions.items():\n",
        "        param_config = BEST_MODELS[param]\n",
        "        param_db_name = param_config['param_db_name']\n",
        "        param_id = param_id_map[param_db_name]\n",
        "        \n",
        "        # Calculate confidence score from R²\n",
        "        # Use R² as proxy for confidence (0 to 1 scale)\n",
        "        # Negative R² becomes 0 confidence\n",
        "        r2 = param_config['r2']\n",
        "        confidence_score = max(0.0, min(1.0, r2))  # Clamp to [0, 1]\n",
        "        \n",
        "        prediction_records.append({\n",
        "            'prediction_run_id': prediction_run_id,\n",
        "            'site_id': site_id,\n",
        "            'date': date,\n",
        "            'type_id': int(surface_type_id),\n",
        "            'param_id': int(param_id),\n",
        "            'param_value': float(y_pred_value[idx]),\n",
        "            'confidence_score': float(confidence_score),\n",
        "            'model_name': param_config['model_name'],\n",
        "        })\n",
        "\n",
        "print(f\"\\n✓ Prepared {len(prediction_records)} prediction records for insertion\")\n",
        "print(f\"\\nSample records:\")\n",
        "for i in range(min(5, len(prediction_records))):\n",
        "    rec = prediction_records[i]\n",
        "    print(f\"  {rec['site_id']} | {rec['date']} | Param ID {rec['param_id']} | \"\n",
        "          f\"Value: {rec['param_value']:.4f} | Confidence: {rec['confidence_score']:.4f} | \"\n",
        "          f\"Model: {rec['model_name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Insert Predictions into Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "INSERTING PREDICTIONS INTO DATABASE\n",
            "======================================================================\n",
            "\n",
            "1. Inserting prediction metadata...\n",
            "   ✓ Metadata inserted successfully\n",
            "\n",
            "2. Inserting 3045 prediction records...\n",
            "   ✓ Batch 1/4 inserted (1000 records)\n",
            "   ✓ Batch 2/4 inserted (1000 records)\n",
            "   ✓ Batch 3/4 inserted (1000 records)\n",
            "   ✓ Batch 4/4 inserted (45 records)\n",
            "\n",
            "======================================================================\n",
            "✓ ALL PREDICTIONS INSERTED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Prediction Run ID: 619d8e03-212c-4145-a6bd-71439df9f28f\n",
            "Total records inserted: 3045\n",
            "Date range: 2025-10-03 to 2025-10-31\n",
            "Sites: 21\n",
            "Parameters: 5\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"INSERTING PREDICTIONS INTO DATABASE\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Step 1: Insert prediction metadata\n",
        "print(f\"\\n1. Inserting prediction metadata...\")\n",
        "try:\n",
        "    response = supabase.table('epa_prediction_metadata').insert(prediction_metadata).execute()\n",
        "    print(f\"   ✓ Metadata inserted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"   ✗ Error inserting metadata: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 2: Bulk insert predictions (in batches for safety)\n",
        "print(f\"\\n2. Inserting {len(prediction_records)} prediction records...\")\n",
        "\n",
        "BATCH_SIZE = 1000  # Insert 1000 records at a time\n",
        "total_batches = (len(prediction_records) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "for batch_idx in range(total_batches):\n",
        "    start_idx = batch_idx * BATCH_SIZE\n",
        "    end_idx = min((batch_idx + 1) * BATCH_SIZE, len(prediction_records))\n",
        "    batch = prediction_records[start_idx:end_idx]\n",
        "    \n",
        "    try:\n",
        "        response = supabase.table('epa_data_prediction').insert(batch).execute()\n",
        "        print(f\"   ✓ Batch {batch_idx+1}/{total_batches} inserted ({len(batch)} records)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ✗ Error inserting batch {batch_idx+1}: {e}\")\n",
        "        raise\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✓ ALL PREDICTIONS INSERTED SUCCESSFULLY!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\nPrediction Run ID: {prediction_run_id}\")\n",
        "print(f\"Total records inserted: {len(prediction_records)}\")\n",
        "print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
        "print(f\"Sites: {len(sites_list)}\")\n",
        "print(f\"Parameters: {len(TARGET_PARAMS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Verify Insertion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying insertion...\n",
            "\n",
            "Sample predictions from database:\n",
            "  site_id        date   param_name  param_value  confidence_score  \\\n",
            "0     716  2025-10-31        CHL_A     0.885489            0.2103   \n",
            "1     716  2025-10-31        DO_mg     7.592948            0.3838   \n",
            "2     716  2025-10-31      N_TOTAL   151.277298            0.2992   \n",
            "3     716  2025-10-31  Temperature    16.712745            0.8982   \n",
            "4     716  2025-10-31         Turb     1.477570            0.4395   \n",
            "5    1229  2025-10-31        CHL_A     0.465133            0.2103   \n",
            "6    1229  2025-10-31        DO_mg     7.801047            0.3838   \n",
            "7    1229  2025-10-31      N_TOTAL   135.583740            0.2992   \n",
            "8    1229  2025-10-31  Temperature    15.946181            0.8982   \n",
            "9    1229  2025-10-31         Turb     0.600315            0.4395   \n",
            "\n",
            "        model_name  \n",
            "0           V2_MLP  \n",
            "1         V4_Ridge  \n",
            "2           V2_MLP  \n",
            "3  V4_RandomForest  \n",
            "4  V4_RandomForest  \n",
            "5           V2_MLP  \n",
            "6         V4_Ridge  \n",
            "7           V2_MLP  \n",
            "8  V4_RandomForest  \n",
            "9  V4_RandomForest  \n",
            "\n",
            "✓ Verified 3045 records in database\n",
            "  Expected: 3045\n",
            "  Match: True\n"
          ]
        }
      ],
      "source": [
        "# Query back the inserted predictions\n",
        "print(f\"\\nVerifying insertion...\")\n",
        "\n",
        "response = supabase.table('v_epa_predictions').select('*').eq('prediction_run_id', prediction_run_id).limit(10).execute()\n",
        "df_verify = pd.DataFrame(response.data)\n",
        "\n",
        "print(f\"\\nSample predictions from database:\")\n",
        "print(df_verify[['site_id', 'date', 'param_name', 'param_value', 'confidence_score', 'model_name']].head(10))\n",
        "\n",
        "# Get count\n",
        "response_count = supabase.table('epa_data_prediction').select('id', count='exact').eq('prediction_run_id', prediction_run_id).execute()\n",
        "db_count = response_count.count\n",
        "\n",
        "print(f\"\\n✓ Verified {db_count} records in database\")\n",
        "print(f\"  Expected: {len(prediction_records)}\")\n",
        "print(f\"  Match: {db_count == len(prediction_records)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "PREDICTION SUMMARY\n",
            "====================================================================================================\n",
            "  Parameter           Model  R² Score  Predictions        Min        Max       Mean        Std\n",
            "      CHL_A          V2_MLP    0.2103          609   0.297403  21.859091   3.368788   5.040491\n",
            "  TURBIDITY V4_RandomForest    0.4395          609   0.455079  26.973292   4.054108   5.644768\n",
            "    DO_MG_L        V4_Ridge    0.3838          609   7.426567  50.039085  10.401955   8.880754\n",
            "    N_TOTAL          V2_MLP    0.2992          609 101.196121 832.927551 305.959412 211.579010\n",
            "TEMPERATURE V4_RandomForest    0.8982          609  14.272797  17.503591  16.141479   0.697973\n",
            "====================================================================================================\n",
            "\n",
            "✅ Inference complete!\n",
            "\n",
            "Next steps:\n",
            "  1. Query predictions via: SELECT * FROM v_epa_predictions WHERE prediction_run_id = '619d8e03-212c-4145-a6bd-71439df9f28f'\n",
            "  2. Wide format view: SELECT * FROM v_epa_measurements_wide_prediction WHERE prediction_run_id = '619d8e03-212c-4145-a6bd-71439df9f28f'\n",
            "  3. Integrate with backend API to serve predictions to frontend\n"
          ]
        }
      ],
      "source": [
        "# Create summary DataFrame\n",
        "summary_data = []\n",
        "\n",
        "for param, config in BEST_MODELS.items():\n",
        "    y_pred = predictions[param]\n",
        "    \n",
        "    summary_data.append({\n",
        "        'Parameter': param.upper(),\n",
        "        'Model': config['model_name'],\n",
        "        'R² Score': config['r2'],\n",
        "        'Predictions': len(y_pred),\n",
        "        'Min': y_pred.min(),\n",
        "        'Max': y_pred.max(),\n",
        "        'Mean': y_pred.mean(),\n",
        "        'Std': y_pred.std(),\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "print(f\"\\n{'='*100}\")\n",
        "print(\"PREDICTION SUMMARY\")\n",
        "print(f\"{'='*100}\")\n",
        "print(df_summary.to_string(index=False))\n",
        "print(f\"{'='*100}\")\n",
        "\n",
        "print(f\"\\n✅ Inference complete!\")\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"  1. Query predictions via: SELECT * FROM v_epa_predictions WHERE prediction_run_id = '{prediction_run_id}'\")\n",
        "print(f\"  2. Wide format view: SELECT * FROM v_epa_measurements_wide_prediction WHERE prediction_run_id = '{prediction_run_id}'\")\n",
        "print(f\"  3. Integrate with backend API to serve predictions to frontend\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
